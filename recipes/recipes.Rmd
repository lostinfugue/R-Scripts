---
title: "recipes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Tutorials I'm Following: 

* https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/


## Load Tools

* rjson

```{r echo = FALSE, message=FALSE, reformat=TRUE}
# install.packages("rjson")
# require(data.table)
require(jsonlite)
require(dplyr)
require(tidyr)
# install.packages("tm")
library(tm)
# install.packages(("SnowballC"))
library(SnowballC) ## for stemming

```

## Recipes Analysis
Looking at data on 40k recipes, using ingredients + labeled cuisines to build a predictive classification model.  Testing model on 10k recipes unlabeled data set.

### Load & Format Data

* **Training Data**: 39774 recipes
* **Test Data**: 9944 recipes

```{r echo = FALSE, message=FALSE, reformat=TRUE}
## load training and testing data
train_file <- "data/train.json"
test_file <- "data/test.json"
train <- jsonlite::fromJSON(train_file)
test <- jsonlite::fromJSON(test_file)

# train <- as_tibble(train)
# test <- as_tibble(test)

#str(train)
#str(test)
```

Flattened Training Data - 1 row per ingredient
```{r echo = FALSE}
train_ingredients <- train %>%
  unnest(ingredients)

train_ingredients
```
Flattened Test Data - 1 row per ingredient
```{r echo = FALSE}

test_ingredients <- test %>%
  unnest(ingredients)

test_ingredients
```



### Summarize Data
```{r echo = FALSE}
summary(train_ingredients)
summary(test_ingredients)

```

#### How many total recipes are there?
```{r echo = FALSE}

train_recipe_count <- train_ingredients %>%
  summarise(n_recipes = n_distinct(id))

as.integer(train_recipe_count)

```

#### What are the different cuisines?  How many recipes are there in the training data?
```{r echo = FALSE}

train_cuisine_recipe_counts <- train_ingredients %>%
  group_by(cuisine) %>%
  summarise(n_recipes = n_distinct(id)) %>%
  arrange(desc(n_recipes))

train_cuisine_recipe_counts

```


#### What are the top ingredients per cuisine? How often do they appear in the training data?
```{r echo = FALSE}
train_ingredient_counts_by_cuisine <- train_ingredients %>%
  group_by(cuisine) %>%
  count(cuisine,ingredients) %>%
  arrange(cuisine, desc(n))

train_ingredient_frequencies_by_cuisine <- 
  merge(train_ingredient_counts_by_cuisine, train_cuisine_recipe_counts) %>%
  mutate(pct_of_cuisine_recipes = n/n_recipes)

train_ingredient_frequencies_by_cuisine %>%
  group_by(cuisine) %>%
  arrange(cuisine, desc(n)) %>%
   top_n(10)

```

#### What are the most distinctive ingredients per cuisine?
```{r echo = FALSE}
train_ingredient_counts <- train_ingredients %>%
  group_by(ingredients) %>%
  count(ingredients) %>%
  arrange(desc(n))

train_ingredient_frequencies <- 
  train_ingredient_counts %>%
  mutate(n_total_recipes = as.integer(train_recipe_count)) %>%
  mutate(pct_of_total_recipes = n/n_total_recipes)


merge(train_ingredient_frequencies_by_cuisine, train_ingredient_frequencies, by="ingredients") %>%
  mutate(pct_diff = round((pct_of_cuisine_recipes - pct_of_total_recipes),2)) %>%
  mutate(pct_of_cuisine_recipes = round(pct_of_cuisine_recipes,2)) %>%
  mutate(pct_of_total_recipes = round(pct_of_total_recipes,2)) %>%
  arrange(desc(pct_diff)) %>%
  # arrange(pct_diff) %>%
  select(ingredients, cuisine, pct_of_cuisine_recipes, pct_of_total_recipes, pct_diff) %>%
  top_n(20)

```


#### Modeling
Combine Training and Test Data into one dataset
```{r}
test$cuisine <- NA
combined <- rbind(train, test)

## make cuisine a factor
# train$cuisine <- as.factor(train$cuisine)
# combined$cuisine <- as.factor(combined$cuisine)
# levels(combined$cuisine)

```


Create corpus
```{r}
# ?VectorSource
# ?Corpus

# 1. create corpus
corpus <- Corpus(VectorSource(combined$ingredients))
## see each document
writeLines(as.character(corpus[[1]]))

# 2. Convert text to lowercase
corpus <- tm_map(corpus, tolower)
## see each document
writeLines(as.character(corpus[1:10]$content))

# 3. Remove vector notation c("","",...)
#create the removeVector content transformer
removeVector <- content_transformer(function(x) {return (gsub("^c\\(|\\)$", "", x))})
corpus <- tm_map(corpus, removeVector)
## see each document
writeLines(as.character(corpus[1:10]$content))

# 4. Replace hyphen with space before removing punctuation
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "\\-", replacement = "")

# 5. Remove Punctuation
corpus <- tm_map(corpus, removePunctuation)
## see each document
writeLines(as.character(corpus[1:10]$content))


# 6.Remove Stopwords
corpus <- tm_map(corpus, removeWords, c(stopwords('english')))
## see each document
writeLines(as.character(corpus[100:110]$content))

# 7. Remove Whitespaces
corpus <- tm_map(corpus, stripWhitespace)
## see each document
writeLines(as.character(corpus[100:110]$content))

# 8. Remove Numbers
corpus <- tm_map(corpus, removeNumbers)
## see each document
writeLines(as.character(corpus[100:110]$content))


# 9. Perform Stemming
corpus <- tm_map(corpus, stemDocument)
## see each document
writeLines(as.character(corpus[100:110]$content))



# 10. For further processing, we’ll create a document matrix where the text will categorized in columns
frequencies <- DocumentTermMatrix(corpus) 
## view sparsity and summary:
frequencies
## view subset of document term matrix
inspect(frequencies[1:2,1000:1005])


# 11. noticed weird characters
# find non a-z chars and replace them if regular words
## 79 ingredients with weird chars
head(grep('[^a-z]+',colnames(frequencies)))

## manually clean these weird characters
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "(®|™|’)", replacement = "")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "è", replacement = "e")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "é", replacement = "e")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "ç", replacement = "c")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "â", replacement = "a")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "í", replacement = "i")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "î", replacement = "i")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "€", replacement = "e")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "ú", replacement = "u")


## 12. Recreate Document Term Matrix after removing weird characters
frequencies <- DocumentTermMatrix(corpus)


# 13. Filter down to words with 3+ characters and 3+ ocurrences
filtered_frequencies <- DocumentTermMatrix(corpus, 
   control=list(wordLengths=c(3, Inf), bounds = list(global = c(3,Inf)))
   )
filtered_frequencies



# 14. Analyze frequent / infrequent terms
# Sort terms by frequency
# freq <- colSums(as.matrix(filtered_frequencies))
# length(freq)
# 
# #create sort order (descending) for ingredient name length
# ord <- order(nchar(names(freq)),decreasing=TRUE)
# 
# #inspect most frequently occurring terms
# freq[head(ord)]   
# 
# #inspect least frequently occurring terms
# freq[tail(ord)]

## find most frequent terms with at least 50 ocurrences
# findFreqTerms(filtered_frequencies,lowfreq=50)
# 
# findAssocs(filtered_frequencies,'chees',0.3)
# findAssocs(filtered_frequencies,'rice',0.15)
# findAssocs(filtered_frequencies,'banana',0.15)
# findAssocs(filtered_frequencies,'tofu',0.15)
# findAssocs(filtered_frequencies,'bok',0.15)
# findAssocs(filtered_frequencies,'cream',0.15)
# findAssocs(filtered_frequencies,'coffe',0.1)
# findAssocs(filtered_frequencies,'egg',0.1)
# findAssocs(filtered_frequencies,'soda',0.1)
# findAssocs(filtered_frequencies,'water',0.1)
# findAssocs(filtered_frequencies,'oatmeal',0.1)
# findAssocs(filtered_frequencies,'salt',0.1)
# findAssocs(filtered_frequencies, c('salt','oil'), corlimit=0.30)
# findAssocs(filtered_frequencies,'pesto',0.1)
# findAssocs(filtered_frequencies, c('pesto','tomato'), corlimit=0.10)



# 15.create data frame with 
df <- as.data.frame(as.matrix(filtered_frequencies))

# how many recipes does each cuisine have in the training set?
table(train$cuisine)


# 16. add cuisine to df, with most common cuisine "italian" for all rows from test dataset
df$cuisine <- as.factor(c(train$cuisine, rep('italian', nrow(test))))

# 17. split data into training and test set
mytrain <- df[1:nrow(train),]
mytest <- df[-(1:nrow(train)),]

# install.packages("xgboost")
library(xgboost)
library(Matrix)



# 18. creating the matrix for training the model (all term columns minus cuisine)
ctrain <- xgb.DMatrix(Matrix(data.matrix(mytrain[,!colnames(mytrain) %in% c('cuisine')])), label = as.numeric(mytrain$cuisine)-1)

# 19. test data set preparation (all term columns minus cuisine)
dtest <- xgb.DMatrix(Matrix(data.matrix(mytest[,!colnames(mytest) %in% c('cuisine')]))) 

# watchlist <- list(train = ctrain, test = dtest)

# train multiclass model using softmax



# had to run `brew install libomp` initially in terminal to get caret installation to work on my mac os
# install.packages("caret", dependencies = TRUE)
# install.packages("doSNOW", dependencies = TRUE)
# library(caret)
# library(doSNOW)
## start cluster
# cl2 <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl2)

# Shutdown cluster
# stopCluster(cl2)

# 20. train first model with 30 iterations using softmax
xgbmodel <- xgboost(data = ctrain, max.depth = 25, eta = 0.3, nround = 60, objective = "multi:softmax", num_class = 20, verbose = 1
                    #, nthread=6 #need to config
                    # , watchlist = watchlist
                    )

?xgboost

# predict 1

# run prediction using trained model on test data set
# returns column with integers 0-19 corresponding to predicted cuisine labels
xgbmodel.predict <- predict(xgbmodel, newdata = ctrain)

# returns string versions of cuisine labels
xgbmodel.predict.text <- levels(mytrain$cuisine)[xgbmodel.predict + 1]

# create a dataframe with original training data + predictions for comparison
predictions <- cbind(predicted_cuisine = xgbmodel.predict.text, train)
head(predictions)

View(predictions[which(predictions$predicted_cuisine != predictions$cuisine), ])

# check accuracy on training set = 98.52416% = 1.475838
1- sum(diag(table(as.integer(as.factor(train$cuisine)), xgbmodel.predict + 1)))/nrow(mytrain) 


## alternative approach
# create a data frame with mapping between cuisine strings and integer values used in model
mappingTable = data.frame(cuisine=levels(mytrain$cuisine),code=seq(1,length(levels(mytrain$cuisine))),stringsAsFactors = FALSE)


pred_xgbmodel <- predict(xgbmodel,dtest)
pred_xgbmodel_submit <- cbind(pred_xgbmodel,test)

# add it to 
pred_xgbmodel_submit$cuisine <- mappingTable$cuisine[match(pred_xgbmodel_submit$pred_xgbmodel,mappingTable$code)]

levels(mytrain$cuisine)


#second model
xgbmodel2 <- xgboost(data = ctrain, max.depth = 20, eta = 0.2, nrounds = 250, objective = "multi:softmax", num_class = 20
                     # , watchlist = watchlist
                     )


#third model
xgbmodel3 <- xgboost(data = ctrain, max.depth = 25, gamma = 2, min_child_weight = 2, eta = 0.1, nround = 250, objective = "multi:softmax", num_class = 20, verbose = 2,
                     # watchlist = watchlist
                     )



```

