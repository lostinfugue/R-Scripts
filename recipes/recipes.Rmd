---
title: "recipes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Tutorials I'm Following: 

* https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/


## Load Tools

* rjson

```{r echo = FALSE, message=FALSE, reformat=TRUE}
# install.packages("rjson")
# require(data.table)
require(jsonlite)
require(dplyr)
require(tidyr)
# install.packages("tm")
library(tm)
# install.packages(("SnowballC"))
library(SnowballC) ## for stemming

```

## Recipes Analysis
Looking at data on 40k recipes, using ingredients + labeled cuisines to build a predictive classification model.  Testing model on 10k recipes unlabeled data set.

### Load & Format Data

* **Training Data**: 39774 recipes
* **Test Data**: 9944 recipes

```{r echo = FALSE, message=FALSE, reformat=TRUE}
## load training and testing data
train_file <- "data/train.json"
test_file <- "data/test.json"
train <- jsonlite::fromJSON(train_file)
test <- jsonlite::fromJSON(test_file)

# train <- as_tibble(train)
# test <- as_tibble(test)

#str(train)
#str(test)
```

Flattened Training Data - 1 row per ingredient
```{r echo = FALSE}
train_ingredients <- train %>%
  unnest(ingredients)

train_ingredients
```
Flattened Test Data - 1 row per ingredient
```{r echo = FALSE}

test_ingredients <- test %>%
  unnest(ingredients)

test_ingredients
```



### Summarize Data
```{r echo = FALSE}
summary(train_ingredients)
summary(test_ingredients)

```

#### How many total recipes are there?
```{r echo = FALSE}

train_recipe_count <- train_ingredients %>%
  summarise(n_recipes = n_distinct(id))

as.integer(train_recipe_count)

```

#### What are the different cuisines?  How many recipes are there in the training data?
```{r echo = FALSE}

train_cuisine_recipe_counts <- train_ingredients %>%
  group_by(cuisine) %>%
  summarise(n_recipes = n_distinct(id)) %>%
  arrange(desc(n_recipes))

train_cuisine_recipe_counts

```


#### What are the top ingredients per cuisine? How often do they appear in the training data?
```{r echo = FALSE}
train_ingredient_counts_by_cuisine <- train_ingredients %>%
  group_by(cuisine) %>%
  count(cuisine,ingredients) %>%
  arrange(cuisine, desc(n))

train_ingredient_frequencies_by_cuisine <- 
  merge(train_ingredient_counts_by_cuisine, train_cuisine_recipe_counts) %>%
  mutate(pct_of_cuisine_recipes = n/n_recipes)

train_ingredient_frequencies_by_cuisine %>%
  group_by(cuisine) %>%
  arrange(cuisine, desc(n)) %>%
   top_n(10)

```

#### What are the most distinctive ingredients per cuisine?
```{r echo = FALSE}
train_ingredient_counts <- train_ingredients %>%
  group_by(ingredients) %>%
  count(ingredients) %>%
  arrange(desc(n))

train_ingredient_frequencies <- 
  train_ingredient_counts %>%
  mutate(n_total_recipes = as.integer(train_recipe_count)) %>%
  mutate(pct_of_total_recipes = n/n_total_recipes)


merge(train_ingredient_frequencies_by_cuisine, train_ingredient_frequencies, by="ingredients") %>%
  mutate(pct_diff = round((pct_of_cuisine_recipes - pct_of_total_recipes),2)) %>%
  mutate(pct_of_cuisine_recipes = round(pct_of_cuisine_recipes,2)) %>%
  mutate(pct_of_total_recipes = round(pct_of_total_recipes,2)) %>%
  arrange(desc(pct_diff)) %>%
  # arrange(pct_diff) %>%
  select(ingredients, cuisine, pct_of_cuisine_recipes, pct_of_total_recipes, pct_diff) %>%
  top_n(20)

```


Combine Training and Test Data into one dataset
```{r}
test$cuisine <- NA
combined <- rbind(train, test)

## make cuisine a factor
# train$cuisine <- as.factor(train$cuisine)
# combined$cuisine <- as.factor(combined$cuisine)
# levels(combined$cuisine)

?tm_map

```


Create corpus
```{r}
# ?VectorSource
# ?Corpus

# 1. create corpus
corpus <- Corpus(VectorSource(combined$ingredients))
corpus

## see each document
writeLines(as.character(corpus[[1]]))

# 2. Convert text to lowercase
# ?tm_map
corpus <- tm_map(corpus, tolower)
corpus
corpus[[1]]
## see each document
writeLines(as.character(corpus[1:10]$content))


# 2a. Remove vector notation c("","",...)

#create the removeVector content transformer
removeVector <- content_transformer(function(x) {return (gsub("^c\\(|\\)$", "", x))})
corpus <- tm_map(corpus, removeVector)
corpus
## see each document
writeLines(as.character(corpus[1:10]$content))

?tm_map

# replace hyphen with space before removing punctuation
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "\\-", replacement = "")

# 3. Remove Punctuation
corpus <- tm_map(corpus, removePunctuation)
corpus[[1]]
## see each document
writeLines(as.character(corpus[1:10]$content))


# 4.Remove Stopwords
corpus <- tm_map(corpus, removeWords, c(stopwords('english')))
corpus[[1]]
## see each document
writeLines(as.character(corpus[100:110]$content))

# 5. Remove Whitespaces
corpus <- tm_map(corpus, stripWhitespace)
corpus[[1]]
## see each document
writeLines(as.character(corpus[100:110]$content))

# 5. Remove Numbers
corpus <- tm_map(corpus, removeNumbers)
corpus[[1]]
## see each document
writeLines(as.character(corpus[100:110]$content))


# 6. Perform Stemming
corpus <- tm_map(corpus, stemDocument)
corpus[[1]]
corpus
## see each document
writeLines(as.character(corpus[100:110]$content))


# 7. After we are done with pre-processing, it is necessary to convert the text into plain text document. This helps in pre-processing documents as text documents.
# corpus <- tm_map(corpus, PlainTextDocument)
# corpus


# 8. For further processing, we’ll create a document matrix where the text will categorized in columns



#document matrix
?DocumentTermMatrix
frequencies <- DocumentTermMatrix(corpus) 
frequencies
inspect(frequencies[1:2,1000:1005])

?grep
inspect(frequencies[1:2,which(colnames(frequencies))])

# noticed weird characters
# find non a-z chars and replace them if regular words
length(colnames(frequencies))
head(grep('[^a-z]+',colnames(frequencies)))
## 79 ingredients with weird chars
colnames(frequencies)[grep('[^a-z]+',colnames(frequencies))]
head(colnames(frequencies)[which(colnames(frequencies) %in% c('gruyer','acai','gruyèr','pate','ragu','pure','creme'))])

corpus <- tm_map(corpus, content_transformer(gsub), pattern = "(®|™|’)", replacement = "")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "è", replacement = "e")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "é", replacement = "e")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "ç", replacement = "c")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "â", replacement = "a")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "í", replacement = "i")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "î", replacement = "i")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "€", replacement = "e")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "ú", replacement = "u")



# "organic®" # could indicate organic brand name, but probably better to just make normal organic word
# "rice®"
# "gruyèr" => "gruyer"
# "açai" --> "acai"
# "tabasco®"
# pâté pâte
# neufchâtel
# white®
# bell®
# "giant™"

#some numbers still pop up

## curly single quote "sheep’"



#organizing frequency of terms
freq <- colSums(as.matrix(frequencies))
length(freq)

#create sort order (descending) for frequency
ord <- order(freq,decreasing=TRUE)

#inspect most frequently occurring terms
freq[head(ord)]   

#inspect least frequently occurring terms
freq[tail(ord)]   

filtered_frequencies <- DocumentTermMatrix(corpus, 
   control=list(wordLengths=c(3, Inf), bounds = list(global = c(3,Inf))))
inspect(filtered_frequencies[1:2,1:15])
filtered_frequencies



#organizing frequency of terms
freq <- colSums(as.matrix(filtered_frequencies))
length(freq)



#create sort order (descending) for ingredient name length
ord <- order(nchar(names(freq)),decreasing=TRUE)

#inspect most frequently occurring terms
freq[head(ord)]   

#inspect least frequently occurring terms
freq[tail(ord)]

filtered_frequencies


findFreqTerms(filtered_frequencies,lowfreq=50)


findAssocs(filtered_frequencies,'chees',0.3)
findAssocs(filtered_frequencies,'rice',0.15)

findAssocs(filtered_frequencies,'banana',0.15)

findAssocs(filtered_frequencies,'tofu',0.15)

findAssocs(filtered_frequencies,'bok',0.15)
findAssocs(filtered_frequencies,'cream',0.15)
findAssocs(filtered_frequencies,'coffe',0.1)

findAssocs(filtered_frequencies,'egg',0.1)

findAssocs(filtered_frequencies,'soda',0.1)

findAssocs(filtered_frequencies,'water',0.1)
findAssocs(filtered_frequencies,'oatmeal',0.1)
findAssocs(filtered_frequencies,'salt',0.1)
findAssocs(filtered_frequencies, c('salt','oil'), corlimit=0.30)
findAssocs(filtered_frequencies,'pesto',0.1)
findAssocs(filtered_frequencies, c('pesto','tomato'), corlimit=0.10)



#create data frame

df <- as.data.frame(as.matrix(filtered_frequencies))

table(train$cuisine)


#add cuisine
df$cuisine <- as.factor(c(train$cuisine, rep('italian', nrow(test))))
# install.packages("gdata")
library(gdata)
cuisine_mapping <- mapLevels(df$cuisine)
cuisine_mapping

# split data into training and test set
mytrain <- df[1:nrow(train),]
mytest <- df[-(1:nrow(train)),]

a <- as.data.frame(cbind(cuisine = train$cuisine, cuisine_id = as.numeric(as.factor(train$cuisine))))
?factor

# install.packages("xgboost")
library(xgboost)
library(Matrix)



# creating the matrix for training the model
ctrain <- xgb.DMatrix(Matrix(data.matrix(mytrain[,!colnames(mytrain) %in% c('cuisine')])), label = as.numeric(mytrain$cuisine)-1)

#Encode the levels as integers
levels(mytrain$cuisine)
?levels

#advanced data set preparation
dtest <- xgb.DMatrix(Matrix(data.matrix(mytest[,!colnames(mytest) %in% c('cuisine')]))) 

watchlist <- list(train = ctrain, test = dtest)

#train multiclass model using softmax


## 200 is too much for my machine?
## Try with 5 cores

# had to run `brew install libomp` initially in terminal to get caret installation to work on my mac os
# install.packages("caret", dependencies = TRUE)
# install.packages("doSNOW", dependencies = TRUE)
library(caret)
library(doSNOW)

  ## start cluster
  cl2 <- makeCluster(6, type = "SOCK")
  registerDoSNOW(cl2)


#first model
xgbmodel <- xgboost(data = ctrain, max.depth = 25, eta = 0.3, nround = 30, objective = "multi:softmax", num_class = 20, verbose = 1
                    # , watchlist = watchlist
                    )

  #Shutdown cluster
  stopCluster(cl2)

mappingTable = data.frame(cuisine=levels(mytrain$cuisine),code=seq(1,length(levels(mytrain$cuisine)))-1,stringsAsFactors = FALSE)

pred_xgbmodel_submit$cuisine <- mappingTable$cuisine[match(pred_xgbmodel_submit$pred_xgbmodel,mappingTable$code)]

  
?xgboost
xgbmodel

pred_xgbmodel <- predict(xgbmodel,dtest)
pred_xgbmodel_submit <- cbind(pred_xgbmodel,test)
(mapLevels(pred_xgbmodel_submit$pred_xgbmodel) <- cuisine_mapping)


?mapLevels


#second model
xgbmodel2 <- xgboost(data = ctrain, max.depth = 20, eta = 0.2, nrounds = 250, objective = "multi:softmax", num_class = 20
                     # , watchlist = watchlist
                     )


#third model
xgbmodel3 <- xgboost(data = ctrain, max.depth = 25, gamma = 2, min_child_weight = 2, eta = 0.1, nround = 250, objective = "multi:softmax", num_class = 20, verbose = 2,
                     # watchlist = watchlist
                     )



```

